{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b11ea92",
   "metadata": {},
   "source": [
    "# Neural Network( MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e1759",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167dda7",
   "metadata": {},
   "source": [
    "* Introduction\n",
    "* Application on dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33061204",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a80082b",
   "metadata": {},
   "source": [
    "The Multi-Layer Perceptron (MLP) is a type of neural network that is widely used in machine learning. It is a feedforward neural network, meaning that the information flows through the network in only one direction, from the input layer to the output layer.As a feedforward neural network, it only allows for one direction of information flow—from the input layer to the output layer.\n",
    "\n",
    "Each layer of neurons in the MLP is completely linked to the layer below it. The model's ultimate output is produced by the output layer, which also receives the model's initial input data. The complicated patterns and relationships in the data are learned by the hidden layers, which also carry out the intermediate computations.\n",
    "\n",
    "Each neuron in the MLP gets input from the layer above, calculates a weighted sum, adds a bias term, and then passes the finished calculation via an activation function to produce an output. During training, the neurons' weights and biases are modified using an optimization approach such stochastic gradient descent.\n",
    "\n",
    "The activation function provides non-linearity into the model, enabling it to learn intricate non-linear correlations in the data. As a result, it is a crucial component of the MLP. The sigmoid function, ReLU function, and hyperbolic tangent function are a few examples of regularly used activation functions.\n",
    "\n",
    "The MLP is an effective machine learning model that may be applied to a variety of applications, such as speech recognition, image classification, and natural language processing. However, training an MLP can be challenging because it needs a lot of labeled data and careful consideration when choosing hyperparameters like the number of hidden layers and neurons in each layer.\n",
    "\n",
    "<img src=\"./Desktop/semester2/577_github/neuron.png\" width=\"200\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18803b5e",
   "metadata": {},
   "source": [
    "## Networks of Neurons"
   ]
  },
  {
   "attachments": {
    "nn.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "979afe53",
   "metadata": {},
   "source": [
    "Neurons are arranged into networks of neurons. A row of neurons is called a layer, and one network can have multiple layers. The architecture of the neurons in the network is often called the network topology.\n",
    "\n",
    "\n",
    "![nn.png](attachment:nn.png)\n",
    "\n",
    "Based on the biological neuron model, we can get the basic structure of the multi-layer perceptron MLP. The most typical MLP includes three layers: input layer, hidden layer and output layer. The different layers of the MLP neural network are fully connected (fully connected This means that any neuron in the previous layer is connected to all neurons in the next layer).\n",
    "\n",
    "It can be seen that neural networks have three main basic elements: weights, biases and activation functions\n",
    "\n",
    "Weights: The strength of the connection between neurons is represented by the weights, and the size of the weights indicates the magnitude of the probability.\n",
    "\n",
    "Bias: The bias is set to correctly classify the samples and is an important parameter in the model, i.e., to ensure that the output values calculated from the inputs cannot be activated randomly.\n",
    "\n",
    "Activation function: It acts as a nonlinear mapping, which can limit the output amplitude of the neuron to a certain range, usually between (-1~1) or  (0~1). The most commonly used activation function is the Sigmoid function, which can map (-∞, +∞) numbers to the range of (0~1).\n",
    "\n",
    "\n",
    "The activation functions are tanh and ReLU. tanh is a deformation of Sigmoid function, the mean value of tanh is 0, which has better effect than Sigmoid in practical application; ReLU is a popular activation function recently, when the input signal is less than 0, the output is 0; when the input signal is greater than 0, the output is equal to the input; which activation function to use depends on the specific situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a1413d",
   "metadata": {},
   "source": [
    "### Input or Visible Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1ba46",
   "metadata": {},
   "source": [
    "The bottom layer that takes input from your dataset is called the visible layer because it is the exposed part of the network. Often a neural network is drawn with a visible layer with one neuron per input value or column in your dataset. These are not neurons as described above but simply pass the input value through to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb85fb39",
   "metadata": {},
   "source": [
    "### Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727f728",
   "metadata": {},
   "source": [
    "Layers after the input layer are called hidden layers because they are not directly exposed to the input. The simplest network structure is to have a single neuron in the hidden layer that directly outputs the value.\n",
    "\n",
    "Given increases in computing power and efficient libraries, very deep neural networks can be constructed. Deep learning can refer to having many hidden layers in your neural network. They are deep because they would have been unimaginably slow to train historically but may take seconds or minutes to train using modern techniques and hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c27c78",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb8f54",
   "metadata": {},
   "source": [
    "The final hidden layer is called the output layer, and it is responsible for outputting a value or vector of values that correspond to the format required for the problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845b2c3",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6918e5eb",
   "metadata": {},
   "source": [
    "<img src=\"./Desktop/semester2/577_github/2.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6c2023",
   "metadata": {},
   "source": [
    "## Application on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c45360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5bbb5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = np.load('kmnist-train-imgs.npz')['arr_0']\n",
    "train_y = np.load('kmnist-train-labels.npz')['arr_0']\n",
    "test_X = np.load('kmnist-test-imgs.npz')['arr_0']\n",
    "test_y = np.load('kmnist-test-labels.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84544b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70df538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data \n",
    "train_X = train_X/np.max(train_X)\n",
    "test_X = test_X/np.max(train_X)\n",
    "\n",
    "# Flatten the training images into coloumn vectors. \n",
    "flat_train_X = []\n",
    "onehot_train_y = []\n",
    "\n",
    "for x, y in zip(train_X, train_y):\n",
    "    flat_train_X.append(x.flatten().reshape(784, 1))\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y][0] = 1.0\n",
    "    onehot_train_y.append(temp_vec)\n",
    "   \n",
    "\n",
    "flat_test_X = []\n",
    "onehot_test_y = []\n",
    "\n",
    "for x, y in zip(test_X, test_y):\n",
    "    flat_test_X.append(x.flatten().reshape(784, 1))\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y] = 1.0\n",
    "    onehot_test_y.append(temp_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d4c1d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fedb13d2e50>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_X[10], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4db035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z)*(1.0 - sigmoid(z))\n",
    "\n",
    "def mse(a, y):\n",
    "    return .5*sum((a[i] - y[i])**2 for i in range(10))[0]\n",
    "\n",
    "def initialize_weights(layers = [784, 60, 60, 10]):\n",
    "    W = [[0.0]]\n",
    "    B = [[0.0]]\n",
    "    for i in range(1, len(layers)):\n",
    "        w_temp = np.random.randn(layers[i], layers[i-1])*np.sqrt(2/layers[i-1])\n",
    "        b_temp = np.random.randn(layers[i], 1)*np.sqrt(2/layers[i-1])\n",
    "    \n",
    "        W.append(w_temp)\n",
    "        B.append(b_temp)\n",
    "    return W, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e530a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W, B = initialize_weights()\n",
    "\n",
    "def forward_pass(W, B, x, predict_vector = False):\n",
    "    Z = [[0.0]]\n",
    "    A = [x]\n",
    "    L = len(W) - 1\n",
    "    for i in range(1, L + 1):\n",
    "        z = W[i] @ A[i-1] + B[i]\n",
    "        Z.append(z)\n",
    "        \n",
    "        a = sigmoid(z)\n",
    "        A.append(a)\n",
    "        \n",
    "    if predict_vector == False:\n",
    "        return Z, A\n",
    "    else:\n",
    "        return A[-1]\n",
    "\n",
    "def predict(W, B, x):\n",
    "    _, A = forward_pass(W, B, x)\n",
    "    return np.argmax(A[-1])\n",
    "\n",
    "y_hat = forward_pass(W, B, flat_train_X[0], predict_vector=True)\n",
    "\n",
    "def MSE(W, B, X, y):\n",
    "    cost = 0.0\n",
    "    k = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        a = forward_pass(W, B, xi, predict_vector = True)\n",
    "        cost += mse(a, yi)\n",
    "        k+=1\n",
    "    return cost/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27315d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetwork(object):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.W, self.B = initialize_weights(layers = self.layers)\n",
    "\n",
    "    def train(self, X_train, y_train, alpha = 0.046, epochs = 4):\n",
    "        self.errors_ = [MSE(self.W, self.B, X_train, y_train)]\n",
    "        print(f\"Starting Cost = {self.errors_[0]}\")\n",
    "\n",
    "        sample_size = len(X_train)\n",
    "        L = len(self.layers) - 1\n",
    "     \n",
    "        for k in range(epochs):\n",
    "            \n",
    "            for xi, yi in zip(X_train, y_train):\n",
    "                # Use the forward pass function defined before\n",
    "                # and find the preactivation and postactivation values.\n",
    "                Z, A = forward_pass(self.W, self.B, xi)\n",
    "\n",
    "                # Store the errors in a dictionary for clear interpretation\n",
    "                # of computation of these values.\n",
    "                deltas = dict()\n",
    "\n",
    "                output_error = (A[L] - yi)*d_sigmoid(Z[L])\n",
    "                deltas[L] = output_error\n",
    "\n",
    "                # Loop from L-1 to 1. Recall the right entry of the range function \n",
    "                # is non-inclusive. \n",
    "                for i in range(L-1, 0, -1):\n",
    "                    \n",
    "                    deltas[i] = (self.W[i+1].T @ deltas[i+1])*d_sigmoid(Z[i])\n",
    "\n",
    "                # Loop over each hidden layer and the output layer to perform gradient \n",
    "                # descent. \n",
    "                for i in range(1, L+1):\n",
    "                    self.W[i] -= alpha*deltas[i] @ A[i-1].T\n",
    "                    self.B[i] -= alpha*deltas[i]\n",
    "\n",
    "            # Show the cost over all training examples\n",
    "            self.errors_.append(MSE(self.W, self.B, X_train, y_train))   \n",
    "            print(f\"{k + 1}-Epoch Cost = {self.errors_[-1]}\")\n",
    "    \n",
    "    def predict(self, xi):\n",
    "        depth = len(self.layers)\n",
    "        _, A = forward_pass(self.W, self.B, xi)\n",
    "        return np.argmax(A[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0acc51b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE(net.W, net.B, flat_train_X, onehot_train_y) = 1.6419271436263168 \n",
      "\n",
      "Starting Cost = 1.6419271436263168\n",
      "1-Epoch Cost = 0.12861987771062058\n",
      "2-Epoch Cost = 0.11149488928621086\n",
      "3-Epoch Cost = 0.10422497502657896\n",
      "4-Epoch Cost = 0.09807589047131851\n"
     ]
    }
   ],
   "source": [
    "net = DenseNetwork(layers = [784, 100, 120, 100, 10])\n",
    "\n",
    "# Check the mean squared error before training \n",
    "print(f\"MSE(net.W, net.B, flat_train_X, onehot_train_y) = {MSE(net.W, net.B, flat_train_X, onehot_train_y)} \\n\")\n",
    "net.train(flat_train_X, onehot_train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c9046",
   "metadata": {},
   "source": [
    "The cost is become smaller and smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3e796a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sf/smhqk9hx489bblvt_p7gst_w0000gn/T/ipykernel_3944/4076384693.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0/(1.0+np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "actual_y = []\n",
    "for i in range(len(flat_test_X)):\n",
    "    prediction = net.predict(flat_test_X[i])\n",
    "    y_pred.append(prediction)\n",
    "    actual_y.append(np.argmax(onehot_test_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ee65d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8327"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred,actual_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d98728bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.84      0.81       935\n",
      "           1       0.96      0.94      0.95      1026\n",
      "           2       0.74      0.75      0.74       991\n",
      "           3       0.83      0.84      0.84       990\n",
      "           4       0.83      0.67      0.74      1249\n",
      "           5       0.75      0.99      0.85       763\n",
      "           6       0.55      0.72      0.62       760\n",
      "           7       0.94      0.86      0.90      1092\n",
      "           8       0.96      0.92      0.94      1052\n",
      "           9       0.96      0.85      0.90      1142\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.83      0.84      0.83     10000\n",
      "weighted avg       0.85      0.83      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report = classification_report(y_pred,actual_y)\n",
    "print(\"\\nClassification Report\")\n",
    "print(model_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee195c",
   "metadata": {},
   "source": [
    "The accuracy score is 0.8327, which is not bad. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
